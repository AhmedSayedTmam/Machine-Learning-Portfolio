{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFO Sightings Implementation and Operations Lab\n",
    "\n",
    "The goal of this notebook is to train and deploy our model into SageMaker online hosting with 1 variant. \n",
    "\n",
    "What we plan on accompishling is the following:\n",
    "1. [Load dataset onto Notebook instance memory from S3](#Step-1:-Load-the-data-from-Amazon-S3)\n",
    "1. [Cleaning, transforming and preparing the dataset](#Step-2:-Cleaning,-transforming-and-preparing-the-dataset)\n",
    "1. [Create and train our model (Linear Learner)](#Step-4:-Creating-and-training-our-model-(Linear-Learner))\n",
    "1. [Deploying the model into SageMaker hosting](#Step-4:-Deploying-the-model-into-SageMaker-hosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's go ahead and import all the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from Amazon S3\n",
    "Let's get the UFO sightings data that is stored in S3 and load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reportedTimestamp</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>eventTime</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>weather</th>\n",
       "      <th>firstName</th>\n",
       "      <th>lastName</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>sighting</th>\n",
       "      <th>physicalEvidence</th>\n",
       "      <th>contact</th>\n",
       "      <th>researchOutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1977-04-04T04:02:23.340Z</td>\n",
       "      <td>1977-03-31</td>\n",
       "      <td>23:46</td>\n",
       "      <td>circle</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>rain</td>\n",
       "      <td>Ila</td>\n",
       "      <td>Bashirian</td>\n",
       "      <td>47.329444</td>\n",
       "      <td>-122.578889</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1982-11-22T02:06:32.019Z</td>\n",
       "      <td>1982-11-15</td>\n",
       "      <td>22:04</td>\n",
       "      <td>disk</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>Eriberto</td>\n",
       "      <td>Runolfsson</td>\n",
       "      <td>52.664913</td>\n",
       "      <td>-1.034894</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992-12-07T19:06:52.482Z</td>\n",
       "      <td>1992-12-07</td>\n",
       "      <td>19:01</td>\n",
       "      <td>circle</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>clear</td>\n",
       "      <td>Miller</td>\n",
       "      <td>Watsica</td>\n",
       "      <td>38.951667</td>\n",
       "      <td>-92.333889</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-24T21:06:34.898Z</td>\n",
       "      <td>2011-02-21</td>\n",
       "      <td>20:56</td>\n",
       "      <td>disk</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>Clifton</td>\n",
       "      <td>Bechtelar</td>\n",
       "      <td>41.496944</td>\n",
       "      <td>-71.367778</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991-03-09T16:18:45.501Z</td>\n",
       "      <td>1991-03-09</td>\n",
       "      <td>11:42</td>\n",
       "      <td>circle</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>mostly cloudy</td>\n",
       "      <td>Jayda</td>\n",
       "      <td>Ebert</td>\n",
       "      <td>47.606389</td>\n",
       "      <td>-122.330833</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          reportedTimestamp   eventDate eventTime   shape  duration  \\\n",
       "0  1977-04-04T04:02:23.340Z  1977-03-31     23:46  circle         4   \n",
       "1  1982-11-22T02:06:32.019Z  1982-11-15     22:04    disk         4   \n",
       "2  1992-12-07T19:06:52.482Z  1992-12-07     19:01  circle        49   \n",
       "3  2011-02-24T21:06:34.898Z  2011-02-21     20:56    disk        13   \n",
       "4  1991-03-09T16:18:45.501Z  1991-03-09     11:42  circle        17   \n",
       "\n",
       "   witnesses        weather firstName    lastName   latitude   longitude  \\\n",
       "0          1           rain       Ila   Bashirian  47.329444 -122.578889   \n",
       "1          1  partly cloudy  Eriberto  Runolfsson  52.664913   -1.034894   \n",
       "2          1          clear    Miller     Watsica  38.951667  -92.333889   \n",
       "3          1  partly cloudy   Clifton   Bechtelar  41.496944  -71.367778   \n",
       "4          1  mostly cloudy     Jayda       Ebert  47.606389 -122.330833   \n",
       "\n",
       "  sighting physicalEvidence contact researchOutcome  \n",
       "0        Y                N       N       explained  \n",
       "1        Y                Y       N       explained  \n",
       "2        Y                N       N       explained  \n",
       "3        Y                N       N       explained  \n",
       "4        Y                N       N       explained  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "bucket='ml-tamam'\n",
    "sub_folder = 'k-means-dataset'\n",
    "data_key = 'ufo_fullset.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, sub_folder, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning, transforming and preparing the dataset\n",
    "This step is so important. It's crucial that we clean and prepare our data before we do anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Let's go ahead and start preparing our dataset by transforming some of the values into the correct data types. Here is what we are going to take care of.\n",
    "1. Convert the `reportedTimestamp` and `eventDate` to a datetime data types.\n",
    "1. Convert the `shape` and `weather` to a category data type.\n",
    "1. Map the `physicalEvidence` and `contact` from 'Y', 'N' to `0`, `1`.\n",
    "1. Convert the `researchOutcome` to a category data type (target attribute).\n",
    "\n",
    "Let's also drop the columns that are not important. \n",
    "1. We can drop `sighting` becuase it is always 'Y' or Yes. \n",
    "1. Let's drop the `firstName` and `lastName` becuase they are not important in determining the `researchOutcome`.\n",
    "1. Let's drop the `reportedTimestamp` becuase when the sighting was reporting isn't going to help us determine the legitimacy of the sighting.\n",
    "1. We would need to create some sort of buckets for the `eventDate` and `eventTime`, like seasons for example, but since the distribution of dates is pretty even, let's go ahead and drop them.\n",
    "\n",
    "Finally, let's apply one-hot encoding\n",
    "1. We need to one-hot both the `weather` attribute and the `shape` attribute. \n",
    "1. We also need to transform or map the researchOutcome (target) attribute into numeric values. This is what the alogrithm is expecting. We can do this by mapping unexplained, explained, and probable to 0, 1, 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the missing values with the most common shape\n",
    "df['shape'] = df['shape'].fillna(df['shape'].value_counts().index[0])\n",
    "\n",
    "df['reportedTimestamp'] = pd.to_datetime(df['reportedTimestamp'])\n",
    "df['eventDate'] = pd.to_datetime(df['eventDate'])\n",
    "\n",
    "df['shape'] = df['shape'].astype('category')\n",
    "df['weather'] = df['weather'].astype('category')\n",
    "\n",
    "df['physicalEvidence'] = df['physicalEvidence'].replace({'Y': 1, 'N': 0})\n",
    "df['contact'] = df['contact'].replace({'Y': 1, 'N': 0})\n",
    "\n",
    "df['researchOutcome'] = df['researchOutcome'].astype('category')\n",
    "\n",
    "df.drop(columns=['firstName', 'lastName', 'sighting', 'reportedTimestamp', 'eventDate', 'eventTime'], inplace=True)\n",
    "\n",
    "# Let's one-hot the weather and shape attribute\n",
    "df = pd.get_dummies(df, columns=['weather', 'shape'])\n",
    "\n",
    "# Let's replace the researchOutcome values with 0, 1, 2 for Unexplained, Explained, and Probable\n",
    "df['researchOutcome'] = df['researchOutcome'].replace({'unexplained': 0, 'explained': 1, 'probable': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>physicalEvidence</th>\n",
       "      <th>contact</th>\n",
       "      <th>researchOutcome</th>\n",
       "      <th>weather_clear</th>\n",
       "      <th>weather_fog</th>\n",
       "      <th>weather_mostly cloudy</th>\n",
       "      <th>...</th>\n",
       "      <th>weather_stormy</th>\n",
       "      <th>shape_box</th>\n",
       "      <th>shape_circle</th>\n",
       "      <th>shape_disk</th>\n",
       "      <th>shape_light</th>\n",
       "      <th>shape_oval</th>\n",
       "      <th>shape_pyramid</th>\n",
       "      <th>shape_sphere</th>\n",
       "      <th>shape_square</th>\n",
       "      <th>shape_triangle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>47.329444</td>\n",
       "      <td>-122.578889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>52.664913</td>\n",
       "      <td>-1.034894</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>38.951667</td>\n",
       "      <td>-92.333889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>41.496944</td>\n",
       "      <td>-71.367778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>47.606389</td>\n",
       "      <td>-122.330833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  witnesses   latitude   longitude  physicalEvidence  contact  \\\n",
       "0         4          1  47.329444 -122.578889                 0        0   \n",
       "1         4          1  52.664913   -1.034894                 1        0   \n",
       "2        49          1  38.951667  -92.333889                 0        0   \n",
       "3        13          1  41.496944  -71.367778                 0        0   \n",
       "4        17          1  47.606389 -122.330833                 0        0   \n",
       "\n",
       "   researchOutcome  weather_clear  weather_fog  weather_mostly cloudy  ...  \\\n",
       "0                1              0            0                      0  ...   \n",
       "1                1              0            0                      0  ...   \n",
       "2                1              1            0                      0  ...   \n",
       "3                1              0            0                      0  ...   \n",
       "4                1              0            0                      1  ...   \n",
       "\n",
       "   weather_stormy  shape_box  shape_circle  shape_disk  shape_light  \\\n",
       "0               0          0             1           0            0   \n",
       "1               0          0             0           1            0   \n",
       "2               0          0             1           0            0   \n",
       "3               0          0             0           1            0   \n",
       "4               0          0             1           0            0   \n",
       "\n",
       "   shape_oval  shape_pyramid  shape_sphere  shape_square  shape_triangle  \n",
       "0           0              0             0             0               0  \n",
       "1           0              0             0             0               0  \n",
       "2           0              0             0             0               0  \n",
       "3           0              0             0             0               0  \n",
       "4           0              0             0             0               0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(18000, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Creating and training our model (Linear Learner)\n",
    "\n",
    "Let's evaluate the Linear Learner algorithm as well. Let's go ahead and randomize the data again and get it ready for the Linear Leaner algorithm. We will also rearrange the columns so it is ready for the algorithm (it expects the first column to be the target attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rand_split = np.random.rand(len(df))\n",
    "train_list = rand_split < 0.8\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "test_list = rand_split >= 0.9\n",
    "\n",
    " # This dataset will be used to train the model.\n",
    "data_train = df[train_list]\n",
    "\n",
    "# This dataset will be used to validate the model.\n",
    "data_val = df[val_list]\n",
    "\n",
    "# This dataset will be used to test the model.\n",
    "data_test = df[test_list]\n",
    "\n",
    "# Breaks the datasets into attribute numpy.ndarray and the same for target attribute.  \n",
    "train_X = data_train.drop(columns='researchOutcome').values\n",
    "train_y = data_train['researchOutcome'].values\n",
    "\n",
    "val_X = data_val.drop(columns='researchOutcome').values\n",
    "val_y = data_val['researchOutcome'].values\n",
    "\n",
    "test_X = data_test.drop(columns='researchOutcome').values\n",
    "test_y = data_test['researchOutcome'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Let's create recordIO file for the training data and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pipe mode recordIO protobuf training data: s3://ml-tamam/implementation_operations_lab/linearlearner_train/ufo_sightings_train_recordIO_protobuf.data\n"
     ]
    }
   ],
   "source": [
    "train_file = 'ufo_sightings_train_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('implementation_operations_lab/linearlearner_train/{}'.format(train_file)).upload_fileobj(f)\n",
    "training_recordIO_protobuf_location = 's3://{}/implementation_operations_lab/linearlearner_train/{}'.format(bucket, train_file)\n",
    "print('The Pipe mode recordIO protobuf training data: {}'.format(training_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create recordIO file for the validation data and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pipe mode recordIO protobuf validation data: s3://ml-tamam/implementation_operations_lab/linearlearner_validation/ufo_sightings_validatioin_recordIO_protobuf.data\n"
     ]
    }
   ],
   "source": [
    "validation_file = 'ufo_sightings_validatioin_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'), val_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('implementation_operations_lab/linearlearner_validation/{}'.format(validation_file)).upload_fileobj(f)\n",
    "validate_recordIO_protobuf_location = 's3://{}/implementation_operations_lab/linearlearner_validation/{}'.format(bucket, validation_file)\n",
    "print('The Pipe mode recordIO protobuf validation data: {}'.format(validate_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Alright we are good to go for the Linear Learner algorithm. Let's get everything we need from the ECR repository to call the Linear Learner algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "container = image_uris.retrieve('linear-learner', boto3.Session().region_name, '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the job name ufo-linear-learner-job-20210626140644\n"
     ]
    }
   ],
   "source": [
    "# Create a training job name\n",
    "job_name = 'ufo-linear-learner-job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print('Here is the job name {}'.format(job_name))\n",
    "\n",
    "# Here is where the model-artifact will be stored\n",
    "output_location = 's3://{}/implementation_operations_lab/linearlearner_output'.format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start building out our model by using the SageMaker Python SDK and passing in everything that is required to create a Linear Learner model.\n",
    "\n",
    "First I like to always create a specific job name. Next, we'll need to specify training parameters.\n",
    "\n",
    "Finally, after everything is included and ready, then we can call the `.fit()` function which specifies the S3 location for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature_dim hyperparameter needs to be set to 22.\n"
     ]
    }
   ],
   "source": [
    "print('The feature_dim hyperparameter needs to be set to {}.'.format(data_train.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceInUse",
     "evalue": "An error occurred (ResourceInUse) when calling the CreateTrainingJob operation: Training job names must be unique within an AWS account and region, and a training job with this name already exists (arn:aws:sagemaker:us-east-1:168402291854:training-job/ufo-linear-learner-job-20210626140644)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceInUse\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8e4c9d39a000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;34m'validation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidate_recordIO_protobuf_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m }\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \"\"\"\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceInUse\u001b[0m: An error occurred (ResourceInUse) when calling the CreateTrainingJob operation: Training job names must be unique within an AWS account and region, and a training job with this name already exists (arn:aws:sagemaker:us-east-1:168402291854:training-job/ufo-linear-learner-job-20210626140644)"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "# Setup the LinearLeaner algorithm from the ECR container\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       instance_count=1, \n",
    "                                       instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       use_spot_instances=True,\n",
    "                                        max_run=900,\n",
    "                                        max_wait=1200,\n",
    "                                       sagemaker_session=sess,\n",
    "                                       input_mode='Pipe')\n",
    "# Setup the hyperparameters\n",
    "linear.set_hyperparameters(feature_dim=22,\n",
    "                           predictor_type='multiclass_classifier',\n",
    "                           num_classes=3\n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here \n",
    "                            # add optimized hyperparmeters here\n",
    "                          )\n",
    "\n",
    "# Launch a training job. This method calls the CreateTrainingJob API call\n",
    "data_channels = {\n",
    "    'train': training_recordIO_protobuf_location,\n",
    "    'validation': validate_recordIO_protobuf_location\n",
    "}\n",
    "linear.fit(data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the location of the trained Linear Learner model: s3://ml-tamam/implementation_operations_lab/linearlearner_output/ufo-linear-learner-job-20210626140644/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print('Here is the location of the trained Linear Learner model: {}/{}/output/model.tar.gz'.format(output_location, job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we have our trained model we can deploy into production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Deploying the model into SageMaker hosting\n",
    "\n",
    "Next, let's deploy the model into SageMaker hosting onto a single m4 instance. We can then use this instance to test the model with the test data that we help out at the beginning of the notebook. We can then evaluate things like accuracy, precision, recall, and f1 score. \n",
    "\n",
    "We can use some fancy libraries to build out a confusion matrix/heatmap to see how accurate our model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Estimator is not associated with a training job",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-083ef148b64e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmulticlass_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \"\"\"\n\u001b[1;32m    919\u001b[0m         \u001b[0mremoved_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_endpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_latest_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_base_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mdefault_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_from_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36m_ensure_latest_training_job\u001b[0;34m(self, error_message)\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0mdelete_endpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoved_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete_endpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Estimator is not associated with a training job"
     ]
    }
   ],
   "source": [
    "multiclass_predictor = linear.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code is just setup code to allow us to draw out nice and pretty confusion matrix/heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None, \n",
    "                          cmap=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "            plt.cm.Greens\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='Actual',\n",
    "           xlabel='Predicted')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.predictor import json_deserializer, csv_serializer\n",
    "\n",
    "# multiclass_predictor.content_type = 'text/csv'\n",
    "multiclass_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "multiclass_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "predictions = []\n",
    "results = multiclass_predictor.predict(test_X)\n",
    "predictions += [r['predicted_label'] for r in results['predictions']]\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.4)\n",
    "\n",
    "y_test = test_y\n",
    "y_pred = predictions\n",
    "\n",
    "class_names = np.array(['Unexplained', 'Explained', 'Probable'])\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix',\n",
    "                      cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test = data_test['researchOutcome']\n",
    "y_pred = predictions\n",
    "scores = precision_recall_fscore_support(y_test, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is: {}'.format(acc))\n",
    "print('Precision is: {}'.format(scores[0]))\n",
    "print('Recall is: {}'.format(scores[1]))\n",
    "print('F1 score is: {}'.format(scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
